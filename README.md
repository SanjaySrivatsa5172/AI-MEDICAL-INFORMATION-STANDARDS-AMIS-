# AI Medical Information Standards (AMIS)

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Standards Version](https://img.shields.io/badge/Standards-v1.0.0-blue.svg)](SPECIFICATION.md)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

<!-- GitHub Topics/Keywords for repository discovery -->
<!-- Add these topics in GitHub: Settings â†’ About â†’ Topics -->

![AI Safety](https://img.shields.io/badge/topic-ai--safety-red)
![Medical AI](https://img.shields.io/badge/topic-medical--ai-blue)
![Health Informatics](https://img.shields.io/badge/topic-health--informatics-green)
![LLM](https://img.shields.io/badge/topic-llm-purple)
![Patient Safety](https://img.shields.io/badge/topic-patient--safety-orange)

**Five foundational standards for AI systems that generate medical information.**

> *"The apparent confidence of any medical statement should not exceed its epistemic warrant."*

---

## What This Repository Addresses

**The risks of medical information generated by AI** â€” including ChatGPT, Google AI Overviews, Gemini, Claude, and other large language models (LLMs) â€” pose serious dangers to patient safety. AI-generated health advice can contain misinformation, hallucinations, and errors that may harm patients who act on incorrect medical guidance.

This repository provides **safety standards to reduce the risks and dangers of AI medical advice**, addressing:

- ğŸš¨ **Risks of AI health misinformation** â€” inaccurate medical facts presented with false confidence
- âš ï¸ **Dangers of LLM medical hallucinations** â€” fabricated citations, wrong dosages, contraindicated advice  
- ğŸ¥ **Patient safety concerns with AI health chatbots** â€” delayed care, missed diagnoses, harmful self-treatment
- ğŸ“Š **Quality control for AI-generated medical content** â€” source verification, uncertainty calibration

**Keywords**: AI medical risks, dangers of AI health advice, LLM medical misinformation, ChatGPT health risks, AI healthcare safety, medical AI hallucinations, patient safety artificial intelligence

---

## Background

In January 2026, a [Guardian investigation](https://www.theguardian.com/technology/2026/jan/02/google-ai-health-summaries-risk) documented systematic failures in AI-generated medical information: dietary advice for pancreatic cancer patients that was "the exact opposite of what should be recommended," laboratory reference ranges presented without critical context, and YouTube cited more frequently than peer-reviewed medical journals.

These failures demonstrate the **risks of relying on AI for medical information** â€” risks that are not bugs to be patched but architectural deficiencies requiring systematic remediation.

This repository provides **implementable standards** for AI medical information systems, grounded in evidence-based medicine principles and informed by a [dharmic philosophical framework](docs/dharmic_framework.md) that distinguishes *constitutive* safety (built into architecture) from *regulatory* safety (bolted on afterward).

---

## The Five Standards

| # | Standard | Core Principle |
|---|----------|----------------|
| 1 | **Literature Review Paradigm** | Treat health queries with systematic evidence appraisal rigour |
| 2 | **Source Quality Hierarchy** | Only peer-reviewed, reputable sources; 5-tier classification |
| 3 | **Mandatory Uncertainty Disclosure** | Confidence calibrated to epistemic warrant + explicit warnings |
| 4 | **Dissent Labelling Without False Certainty** | Label heterodox views; avoid dogma in either direction |
| 5 | **Therapeutic Advice Requires Physician Evaluation** | AI informs; physicians prescribe |

See [SPECIFICATION.md](SPECIFICATION.md) for formal definitions.

---

## Quick Start

### 1. Validate AI Output Against Standards

```python
from implementation.python.validator import AMISValidator

validator = AMISValidator()
result = validator.validate(ai_output, query_type="health")

if not result.compliant:
    print(f"Violations: {result.violations}")
    print(f"Recommendations: {result.recommendations}")
```

### 2. Classify Source Quality

```python
from implementation.python.source_classifier import SourceClassifier

classifier = SourceClassifier()
tier = classifier.classify("https://www.cochranelibrary.com/...")
# Returns: Tier(level=1, name="Systematic Reviews", usage="Primary weight")
```

### 3. Analyze Harm Cascade

```python
from implementation.python.harm_analyzer import HarmCascadeAnalyzer

analyzer = HarmCascadeAnalyzer()
risks = analyzer.analyze(
    content="For liver function, normal ALT is 7-56 U/L",
    context="Patient query about blood test results"
)
# Returns risk assessment across 4 dimensions
```

### 4. Implement via System Prompt

Copy [implementation/prompts/system_prompt_template.md](implementation/prompts/system_prompt_template.md) into your LLM system prompt.

---

## Repository Structure

```
ai-medical-information-standards/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ SPECIFICATION.md                   # Formal standards specification
â”œâ”€â”€ LICENSE                            # CC-BY-4.0
â”‚
â”œâ”€â”€ standards/                         # Machine-readable specifications
â”‚   â”œâ”€â”€ source_hierarchy.yaml          # 5-tier hierarchy
â”‚   â”œâ”€â”€ uncertainty_calibration.yaml   # Confidence thresholds
â”‚   â”œâ”€â”€ harm_cascade.json              # Decision tree
â”‚   â””â”€â”€ validation_schema.json         # JSON Schema for compliance
â”‚
â”œâ”€â”€ implementation/                    # Reference implementations
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ validator.py               # Compliance checker
â”‚   â”‚   â”œâ”€â”€ source_classifier.py       # Tier classification
â”‚   â”‚   â””â”€â”€ harm_analyzer.py           # Cascade analysis
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ system_prompt_template.md  # LLM system prompt
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ extended_rationale.md          # Full philosophical grounding
â”‚   â”œâ”€â”€ dharmic_framework.md           # Constitutive vs regulatory
â”‚   â”œâ”€â”€ implementation_guide.md        # Adoption guide
â”‚   â””â”€â”€ compliance_checklist.md        # Self-assessment
â”‚
â””â”€â”€ examples/
    â”œâ”€â”€ compliant_output.md            # What good looks like
    â””â”€â”€ non_compliant_output.md        # Documented failures
```

---

## Key Concepts

### Constitutive vs Regulatory Safety

| Aspect | Regulatory (Bolt-on) | Constitutive (Built-in) |
|--------|---------------------|------------------------|
| **Mechanism** | Monitor outputs â†’ filter violations | Harmful pathways absent from architecture |
| **Analogy** | Censorship | Grammar |
| **Question** | "How do we catch bad outputs?" | "Why would bad outputs arise?" |
| **Kill switch** | Required | Unnecessary |

This framework, derived from dharmic philosophy (*ahiá¹ƒsÄ*, *pramÄá¹‡a*, *viveka*), argues that the five standards represent **constitutive** safety measuresâ€”they don't filter harmful outputs but prevent harmful reasoning pathways from existing.

See [docs/dharmic_framework.md](docs/dharmic_framework.md) for full exposition.

### Harm Cascade Analysis

Before generating medical information, systems should trace potential harms across four dimensions:

1. **Direct harm**: Could this cause physical harm if acted upon?
2. **Indirect harm**: Could this lead to delayed necessary care?
3. **Epistemic harm**: Could this distort understanding in compounding ways?
4. **Systemic harm**: Could this degrade public health understanding at scale?

---

## Citation

If you use these standards in research or implementation, please cite:

```bibtex
@article{srivatsa2026standards,
  title={Standards for AI-generated medical information: lessons from documented failures},
  author={Srivatsa, S. Sanjay},
  journal={The Lancet Digital Health},
  year={2026},
  note={Comment}
}
```

---

## Contributing

We welcome contributions that:
- Extend machine-readable specifications
- Add implementations in other languages
- Provide additional compliant/non-compliant examples
- Translate documentation

Please open an issue before submitting significant changes.

---

## Related Work

- [EXSTO ERGO SUM: A Dharmic Framework for AI Alignment](https://github.com/placeholder) â€” The broader philosophical framework
- [Guardian Investigation (Jan 2026)](https://www.theguardian.com/technology/2026/jan/02/google-ai-health-summaries-risk) â€” Empirical foundation
- [GRADE Working Group](https://www.gradeworkinggroup.org/) â€” Evidence quality assessment
- [Cochrane Collaboration](https://www.cochranelibrary.com/) â€” Systematic review methodology

---

## GitHub Topics

**âš ï¸ IMPORTANT: Set your repository "About" description for search discoverability!**

Click the âš™ï¸ gear next to "About" and paste this description:
```
Safety standards addressing the risks and dangers of medical information generated by AI (ChatGPT, Gemini, LLMs). Reduces AI health misinformation, hallucinations, and patient safety risks. Evidence-based guidelines for AI medical advice.
```

### Add These Topics (in Settings â†’ About â†’ Topics)
```
ai-safety
medical-ai
healthcare-ai
patient-safety
health-informatics
llm
large-language-models
```

### Domain Topics
```
digital-health
medical-informatics
clinical-informatics
evidence-based-medicine
health-misinformation
ai-ethics
```

### Technical Topics
```
python
validation
compliance
standards
machine-learning
artificial-intelligence
nlp
```

### Methodological Topics
```
systematic-review
medical-guidelines
quality-assurance
risk-assessment
```

**To add topics**: To further explore the issue of AI safety go to https://github.com/SanjaySrivatsa5172/EXSTO-ERGO-SUM.git

---

**Maintainer**: S. Sanjay Srivatsa, MD | Heart Artery and Vein Center, Fresno, California | email: drsanjaysrivatsa@gmail.com

*Accuracy and appropriate epistemic humility in health information are not optional design featuresâ€”they are prerequisites for AI systems that interact with human health and lives.*

---

<!-- 
SEO KEYWORDS FOR GITHUB SEARCH (hidden section)
This section helps GitHub index this repository for common searches.

Search phrases this repository addresses:
- risks of medical information generated by AI
- dangers of AI health advice  
- AI medical misinformation
- ChatGPT medical risks
- LLM health information dangers
- AI healthcare safety standards
- medical AI hallucinations
- patient safety AI chatbots
- Google AI health risks
- artificial intelligence medical errors
- AI generated health advice risks
- dangers of using ChatGPT for medical advice
- LLM medical accuracy problems
- AI health misinformation risks
- medical AI safety guidelines
- risks of AI in healthcare
- AI medical information accuracy
- ChatGPT health misinformation
- Gemini medical advice risks
- Claude AI medical safety
-->
